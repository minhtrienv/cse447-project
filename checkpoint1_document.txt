CSE 447 Project - Checkpoint 1 Document
========================================

DATASET
-------

We use Wikipedia as our primary training corpus, accessed via Kaggle:

1. **Wikipedia Dataset (Primary)**
   - Source: Kaggle - "jjinho/wikipedia-20230701"
   - Downloaded using kagglehub Python library
   - Contains high-quality, diverse English text
   - Covers wide range of topics including space, science, and general knowledge
   - Well-suited for training language models due to clean formatting

   Why Wikipedia for a space communication system:
   - Contains extensive articles on space exploration, ISS, astronauts
   - Includes scientific terminology astronauts would use
   - Provides general language patterns for everyday communication
   - High-quality, grammatically correct text

2. **Data Access Method**
   - Library: kagglehub (pip install kagglehub)
   - Code: `kagglehub.dataset_download("jjinho/wikipedia-20230701")`
   - Automatically cached after first download

Data Preprocessing:
- Convert all text to character sequences
- Limit training data to ~1 million characters for efficient training
- Skip very short lines (< 50 characters)
- Focus on printable ASCII characters with fallback for unknowns
- Maintain natural text patterns and punctuation


METHOD
------

We implement a character-level LSTM (Long Short-Term Memory) neural network:

**Architecture:**
- Type: Character-level LSTM language model
- Framework: PyTorch
- Structure:
  - Embedding layer: Maps characters to 64-dimensional vectors
  - LSTM layer: 128 hidden units, 1 layer
  - Output layer: Linear projection to vocabulary size
  - Softmax for probability distribution over next character

**How it works:**
1. Input: A sequence of characters (the context/prefix)
2. Each character is converted to an embedding vector
3. LSTM processes the sequence, maintaining hidden state
4. Final hidden state is projected to vocabulary size
5. Top 3 most probable characters are returned as predictions

**Training:**
- Optimizer: Adam with learning rate 0.001
- Loss function: Cross-entropy loss
- Batch size: 64
- Context window: 32 characters
- Training data: ~1 million characters from Wikipedia

**Advantages of LSTM for this task:**
- Captures long-range dependencies in text
- Learns patterns from large text corpora
- Handles variable-length input contexts
- Efficient inference (single forward pass)

**Implementation Details:**
- Language: Python 3.x
- Dependencies: PyTorch, kagglehub
- Device: Automatically uses GPU if available, falls back to CPU
- Checkpoint format: PyTorch .pt files for model weights, JSON for config

**Fallback Strategy:**
- If model is not trained, uses English letter frequency defaults
- Ensures system always produces valid output (3 characters per prediction)
